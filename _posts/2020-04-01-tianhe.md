---
layout: post
title: 天河二号CPU/GPU集群简明使用指南
tag: [tools]
---

本文记录[天河二号](http://www.nscc-gz.cn/)CPU/GPU集群的使用方式。

<!--more-->

## 连接登录
使用天河二号**内部VPN**连入，并用SSH登录集群。

## 天河的系统架构
天河二号采用的是**分布式架构**，一个账户可以同时管理多个节点(node)共同参与计算。

上面用SSH连入的是**登录节点**，实际计算发生是在**计算节点**
* 登录节点用于文件上传/下载、编译
* 计算节点用于实际的程序运行

天河二号的每个节点都有自己的**独立内存**，但是有**共用的硬盘**/文件系统（通过mount挂载），故在登录节点上传的文件，可以在各个计算节点访问到，而无需登录到计算节点重新上传。

但需要注意，天河**并非共享内存**架构，如果采用多个节点进行计算，则每个节点加载入内存的内容是不一样的，需要显式通过MPI管理通信以及数据一致性。

## 常用指令
### CPU集群
下面示例在登录节点上编译C++程序并在计算结点运行并行程序的方法，需先登录CPU集群。
```bash
############ 登录节点 ###########
$ module avail # 查看可用模块
gcc/4.7.4                            intel-compilers/11.1
gcc/4.8.4                            intel-compilers/13.0.0
gcc/4.8.5                            intel-compilers/14.0.2
gcc/4.9.2                            intel-compilers/15.0.1
gcc/5.2.0                            intel-compilers/2017_update4
gcc/5.3.0                            intel-compilers/2018
gcc/5.4.0

# 登录节点默认为gcc 4.4.6编译器，根据个人需求加载模块进行更换
$ module load intel-compilers/2018 # 这里使用Intel的icpc编译器进行编译
$ icpc --version
icpc (ICC) 18.0.0 20170811
Copyright (C) 1985-2017 Intel Corporation.  All rights reserved.

$ icpc -fopenmp -std=c++11 matmul.cpp -o matmul # 编译

# 不要直接在登录节点./matmul！
# 有三种方式将计算任务迁移到计算节点，见下

# 天河二号由银河系列超级计算机继承而来，因此指令前缀均为yh
$ yhinfo # 查看集群节点状态
# alloc代表被占用，idle代表空闲
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
free         up   infinite    757  alloc cn[9600-9622,...]
free         up   infinite     18   idle cn[9807,9894,...]

$ yhalloc -N 1 -p free # -N指出分配节点数目，-p指出分区(上面的PARTITION)
yhalloc: Granted job allocation 14248255

$ yhq # 查看分配的节点信息
   JOBID PARTITION     NAME         USER ST       TIME  NODES NODELIST(REASON)
14248255      free     bash         sysu  R       0:30      1 cn9603

# 方式一：交互式运行
$ yhrun -n 1 -N 1 -p free ./matmul # 任务数，任务需要的节点数，分区，执行指令
# 更多指令可通过yhrun -h查看

# 方式二：登录到计算节点执行（对于CPU集群不建议此项操作）
$ ssh cn9603 # 登录到计算节点（只有分配了才能登录）

############ 计算节点 ##########
$ ./matmul # 可能会有动态库未加载，需返回登录节点重新操作，详情参见详细用户手册

$ exit # 返回登录节点
##################################

$ yhcancel 14248255 # 结束计算任务（如果退出登录节点的SSH，所有占用资源都会被释放），ID号见yhq

# 方式三：批处理方式
# 该方式无需提前分配计算节点，分配调度交由底层系统完成
$ vim mybash.sh
# !/bin/bash
yhrun -n 1 -N 1 -p free ./matmul
$ chmod +x mybash # 为mybash.sh提供可执行权限
$ yhbatch -N 1 -p free ./mybash.sh
Submitted batch job 14248644
$ cat slurm-14248644.out # 执行完后会在本文件夹生成slurm输出文件
...
```

### GPU集群
下面示例在登录节点上分配计算资源并进入计算节点利用PyTorch进行深度学习计算的方法，需先登录GPU集群。
```bash
############ 登录节点 ###########
# 天河二号由银河系列超级计算机继承而来，因此指令前缀均为yh
$ yhinfo # 查看集群节点状态
# alloc代表被占用，idle代表空闲
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
gpu_v100     up   infinite     31  alloc gpu[11-13,15-18,20-22,24,26-27,29-30,34,36,38-42,44-45,47-51,54,56]
gpu_v100     up   infinite      1   idle gpu9

$ yhalloc -N 1 -p gpu_v100 # -N指出分配节点数目，-p指出分区(上面的PARTITION)
yhalloc: Pending job allocation 139022
yhalloc: job 139022 queued and waiting for resources
yhalloc: job 139022 has been allocated resources
yhalloc: Granted job allocation 139022

$ yhq # 查看分配的节点信息
 JOBID PARTITION     NAME         USER ST       TIME  NODES NODELIST(REASON)
139022  gpu_v100     bash         sysu  R       0:25      1 gpu9

$ ssh gpu9 # 登录到计算节点（只有分配了才能登录）

############ 计算节点 ###########
$ module avail # 查看可用模块
--------------------------------------- /app/modulefiles/compiler ----------------------------------------
gcc/4.9.4  gcc/5.5.0  gcc/6.5.0  gcc/9.2.0  intel/15.0.1  intel/18.0.1

------------------------------------------ /app/modulefiles/MPI ------------------------------------------
IMPI/5.0.2.044-icc-15.0.1   mvapich2/2.3.2-gcc-5.5.0   mvapich2/2.3.2-icc-18.0.1
IMPI/2018.1.163-icc-18.0.1  mvapich2/2.3.2-gcc-6.5.0   openmpi/3.1.4-icc-15.0.1
mvapich2/2.3.2-gcc-4.8.5    mvapich2/2.3.2-gcc-9.2.0   openmpi/3.1.4-icc-18.0.1
mvapich2/2.3.2-gcc-4.9.4    mvapich2/2.3.2-icc-15.0.1

---------------------------------------- /app/modulefiles/library ----------------------------------------
gmp/4.2.4  mpc/0.8.1  mpfr/2.4.2  proxy/1.0

------------------------------------- /app/modulefiles_GPU/compiler --------------------------------------
CUDA/10.0  CUDA/10.1.2  PGIcompiler/17.1

---------------------------------------- /app/modulefiles_GPU/MPI ----------------------------------------
mvapich2/2.3rc2-gcc-CUDA-10.1.2  openmpi/1.10.2-pgi-17.1

------------------------------------ /app/modulefiles_GPU/application ------------------------------------
anaconda2/5.3.1         cmake/3.14.3-gcc-4.8.5           python/2.7.15_anaconda2
anaconda3/5.3.1         deeplearning/19Q3-CUDA10.0-py36  python/3.6.7_anaconda3
bazel/0.29.1            jdk/10.0.1                       PyTorch/1.2.0-CUDA10.0-py3.6
cmake/3.9.0-gcc-4.8.5   keras/2.3.0-CUDA10.1-py3.6       TensorFlow/1.13.2-gpu-py3.6-cuda10
cmake/3.10.2-gcc-4.8.5  protobuf/3.9.2-py27              TensorFlow/2.0.0rc2-gpu-py3.6-cuda10

-------------------------------------- /app/modulefiles_GPU/library --------------------------------------
boost/1.59.0-gcc-4.8.5  cudnn/7.6.4-CUDA10.0  gmp/4.2.4  mpfr/2.4.2            nccl/2.4.6-cuda-10.1
cudnn/7.4.1-CUDA10.0    cudnn/7.6.4-CUDA10.1  mpc/0.8.1  nccl/2.4.6-cuda-10.0

$ module load anaconda3/5.3.1 # 加载模块
$ module load CUDA/10.0
$ module load cudnn/7.6.4-CUDA10.0
$ module load PyTorch/1.2.0-CUDA10.0-py3.6

$ python do_something.py # 执行任务

# 或交互式运行
$ python
>>> import torch
>>> torch.cuda.is_available()
True

$ nvidia-smi # 查看GPU资源

$ exit # 返回登录节点
##################################

$ yhcancel 139022 # 结束计算任务（如果退出登录节点的SSH，所有占用资源都会被释放），ID号见yhq

# 或者直接在登录节点用批处理提交作业任务
# 与mpirun类似
# yhrun [options] program
$ yhrun -n 1 -N 2 -p gpu_v100 ./test.py

# 批处理作业
$ vi mybash.sh
# !/bin/bash
yhrun -n 1 -N 2 -p ./test.py
$ chmod +x mybash # 为mybash.sh提供可执行权限
$ yhbatch -N 2 -p gpu_v100 ./mybash.sh
```

## 查看硬件配置
```bash
# 需登录计算节点操作，否则查看的是登录节点的资源情况
$ top
$ cat /etc/redhat-release
$ cat /proc/cpuinfo
# cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l
# physical id 物理CPU个数
# cat /proc/cpuinfo| grep "cpu cores"| uniq
# cpu cores 核数
# cat /proc/cpuinfo| grep "processor"| wc -l
# processor 总逻辑CPU个数
$ cat /proc/meminfo
$ nvidia-smi # gpuinfo
```

天河CPU/GPU集群每节点均为配备2\*Intel Xeon CPU，CPU节点内存64G，GPU节点内存256G。