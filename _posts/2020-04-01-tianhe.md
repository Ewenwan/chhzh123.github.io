---
layout: post
title: 天河二号CPU/GPU集群简明使用指南
tag: [tools]
---

本文记录[天河二号](http://www.nscc-gz.cn/)CPU/GPU集群的使用方式。

<!--more-->

## 连接登录
使用天河二号**内部VPN**连入，并用SSH登录集群。

## 天河的系统架构
天河二号采用的是**分布式架构**，一个账户可以同时管理多个节点(node)共同参与计算。

上面用SSH连入的是**登录节点**，实际计算发生是在**计算节点**
* 登录节点用于文件上传/下载、编译
* 计算节点用于实际的程序运行

天河二号的每个节点都有自己的**独立内存**，但是有**共用的硬盘**/文件系统（通过mount挂载），故在登录节点上传的文件，可以在各个计算节点访问到，而无需登录到计算节点重新上传。

但需要注意，天河**并非共享内存**架构，如果采用多个节点进行计算，则每个节点加载入内存的内容是不一样的，需要显式通过MPI管理通信以及数据一致性。

## 常用指令
```bash
# 天河二号由银河系列超级计算机继承而来，因此指令前缀均为yh
$ yhinfo # 查看集群节点状态
# alloc代表被占用，idle代表空闲
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
gpu_v100     up   infinite     31  alloc gpu[11-13,15-18,20-22,24,26-27,29-30,34,36,38-42,44-45,47-51,54,56]
gpu_v100     up   infinite      1   idle gpu9

$ yhalloc -N 1 -p gpu_v100 # -N指出分配节点数目，-p指出分区(上面的PARTITION)
yhalloc: Pending job allocation 139022
yhalloc: job 139022 queued and waiting for resources
yhalloc: job 139022 has been allocated resources
yhalloc: Granted job allocation 139022

$ yhq # 查看分配的节点信息
 JOBID PARTITION     NAME         USER ST       TIME  NODES NODELIST(REASON)
139022  gpu_v100     bash         sysu  R       0:25      1 gpu9

$ ssh gpu9 # 登录到计算节点（只有分配了才能登录）

############ 计算结点 ###########
$ module avail # 查看可用模块
--------------------------------------- /app/modulefiles/compiler ----------------------------------------
gcc/4.9.4  gcc/5.5.0  gcc/6.5.0  gcc/9.2.0  intel/15.0.1  intel/18.0.1

------------------------------------------ /app/modulefiles/MPI ------------------------------------------
IMPI/5.0.2.044-icc-15.0.1   mvapich2/2.3.2-gcc-5.5.0   mvapich2/2.3.2-icc-18.0.1
IMPI/2018.1.163-icc-18.0.1  mvapich2/2.3.2-gcc-6.5.0   openmpi/3.1.4-icc-15.0.1
mvapich2/2.3.2-gcc-4.8.5    mvapich2/2.3.2-gcc-9.2.0   openmpi/3.1.4-icc-18.0.1
mvapich2/2.3.2-gcc-4.9.4    mvapich2/2.3.2-icc-15.0.1

---------------------------------------- /app/modulefiles/library ----------------------------------------
gmp/4.2.4  mpc/0.8.1  mpfr/2.4.2  proxy/1.0

------------------------------------- /app/modulefiles_GPU/compiler --------------------------------------
CUDA/10.0  CUDA/10.1.2  PGIcompiler/17.1

---------------------------------------- /app/modulefiles_GPU/MPI ----------------------------------------
mvapich2/2.3rc2-gcc-CUDA-10.1.2  openmpi/1.10.2-pgi-17.1

------------------------------------ /app/modulefiles_GPU/application ------------------------------------
anaconda2/5.3.1         cmake/3.14.3-gcc-4.8.5           python/2.7.15_anaconda2
anaconda3/5.3.1         deeplearning/19Q3-CUDA10.0-py36  python/3.6.7_anaconda3
bazel/0.29.1            jdk/10.0.1                       PyTorch/1.2.0-CUDA10.0-py3.6
cmake/3.9.0-gcc-4.8.5   keras/2.3.0-CUDA10.1-py3.6       TensorFlow/1.13.2-gpu-py3.6-cuda10
cmake/3.10.2-gcc-4.8.5  protobuf/3.9.2-py27              TensorFlow/2.0.0rc2-gpu-py3.6-cuda10

-------------------------------------- /app/modulefiles_GPU/library --------------------------------------
boost/1.59.0-gcc-4.8.5  cudnn/7.6.4-CUDA10.0  gmp/4.2.4  mpfr/2.4.2            nccl/2.4.6-cuda-10.1
cudnn/7.4.1-CUDA10.0    cudnn/7.6.4-CUDA10.1  mpc/0.8.1  nccl/2.4.6-cuda-10.0

$ module load anaconda3/5.3.1 # 加载模块
$ module load CUDA/10.0
$ module load cudnn/7.6.4-CUDA10.0
$ module load PyTorch/1.2.0-CUDA10.0-py3.6

$ python do_something.py # 执行任务

$ exit # 返回登录节点
##################################

$ yhcancel 139022 # 结束计算任务（如果退出登录节点的SSH，所有占用资源都会被释放），ID号见yhq

# 或者直接在登录节点用批处理提交作业任务
# 与mpirun类似
# yhrun [options] program
$ yhrun -c 24 -N 2 -p work ls

# 批处理作业
$ vi mybash.sh
# !/bin/bash
yhrun -n 4 -N 4 -p MIC hostname
$ chmod +x mybash # 为mybash.sh提供可执行权限
$ yhbatch -N 4 -p MIC ./mybash.sh
```

## 查看硬件配置
```bash
# 需登录计算节点操作，否则查看的是登录节点的资源情况
$ top
$ cat /etc/redhat-release
$ cat /proc/cpuinfo
# cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l
# physical id 物理CPU个数
# cat /proc/cpuinfo| grep "cpu cores"| uniq
# cpu cores 核数
# cat /proc/cpuinfo| grep "processor"| wc -l
# processor 总逻辑CPU个数
$ cat /proc/meminfo
$ nvidia-smi # gpuinfo
```

天河CPU/GPU集群每节点均为配备2\*Intel Xeon CPU，CPU节点内存64G，GPU节点内存256G。