---
layout: post
title: 图表示学习（2）-图神经网络
tags: [dl,graph]
---

这是**图表示学习(representation learning)的第二部分——图神经网络(graph neural network, gnn)**，主要涉及GCN [ICLR'17]和GraphSAGE [NeurIPS'17]两篇论文。

<!--more-->

关于图数据挖掘/表示学习的内容强烈建议去看Stanford [Jure Leskovec](https://cs.stanford.edu/people/jure/)的[Tutorial - Representation Learning on Networks (WWW'18)](http://snap.stanford.edu/proj/embeddings-www/)。

[前一节]({% post_url 2020-02-06-graph-embedding %})的图嵌入方法都是非常浅(shallow)的，通过学习得到一个矩阵/查找表，来得到结点的嵌入向量。但是这种方法有以下几点缺陷：
* 需要$O(\vert V\vert)$的参数量：没有任何参数的共享，所有结点都只有它自己的嵌入向量
* 内在传导(transductive)：很难生成一个没见过的结点的嵌入
* 无监督方法：没有将结点特征有机整合，而只是关注于图结构

因此我们希望采用更深的方法来得到结点嵌入，最好是直接学习得到一个连续函数，而不是表格映射（这个发展路径其实跟当年强化学习到深度强化学习的过程是类似的）。

核心思想是**从邻居结点整合特征**，通过黑箱网络，对输出拟合。如下图所示，0层即输入层，也即输入特征。

{% include image.html fig="Graph/grl-layers.png" width="80" %}

所以关键在于黑箱中是什么，也即**聚合信息**的方法。

最简单的方式，取平均然后过神经网络。

{% include image.html fig="Graph/grl-basic-math.png" width="80" %}

或用矩阵的形式表示（$H^{(0)}=X$）

$$H^{(k+1)}=f(H^{(k)},A)=\sigma(AH^{(k)}W^{(k)})$$

其中$A\in\mathbb{R}^{n\times n}$为邻接阵，$H\in\mathbb{R}^{n\times d_{in}}$为特征阵（注意这里一行为一个结点的嵌入向量，与前面的向量写法不同），$W\in\mathbb{R}^{d_{in}\times d_{out}}$为每一层的训练权重矩阵（注意到这里是**全层共享参数**）。这里可划分为两个步骤：**消息聚合**与**特征更新**。

第一部分**消息聚合**，即$A$与$H$左乘（稀疏矩阵乘稠密矩阵，图计算），相当于考虑$A$的第$i$行，即第$i$个结点$v_i$，与其邻接的结点的第$j$个特征进行聚合，得到$(i,j)$矩阵元素。

$$(AH^{(k)})_{ij}=\sum_{l}a_{il}h_{lj}=\sum_{v_l\in \mathcal{N}(v_i)}h_{v_l j}
\implies \mathbf{h}_i=\sum_{v_l\in \mathcal{N}(v_i)}\mathbf{h}_{l}$$

第二部分**特征更新**，将聚合的特征与参数矩阵相乘（稠密阵乘稠密阵，深度学习）

$$\mathbf{h}_i^{(k+1)}=\left(\sum_{v_l\in \mathcal{N}(v_i)}\mathbf{h}_{l}^{(k)}\right)W^{(k)}
=\sum_{v_l\in \mathcal{N}(v_i)}\mathbf{h}_{l}^{(k)}W^{(k)}$$

由于矩阵乘符合**结合律**，故消息聚合与特征更新顺序可以互换。
如果先聚合再算更新，则对于结点$i$来说需要做$d_i$次维度$d_{in}$的加法与$1$次乘法$d_{in}\mapsto d_{out}$，由握手定理，全图一共是$2|E|$次加法与$|V|$次矩阵向量乘（或$1$次矩阵矩阵乘）；而先更新再聚合，对于结点$i$需要做$d_i$次维度$d_{out}$的加法与$d_i$次乘法，全图则是$O(|E|)$的时间复杂度。但事实上由于邻居结点会被多个结点共享，因此用矩阵矩阵乘提前算出$H^{(k)}W^{(k)}$，再去索引聚合，乘法次数同样是$1$次。

这里就可以采用前面的无监督方法作为损失函数，或者用有监督的模型来构造。

总的来说，GNN的模型构造分为以下四步走：
1. 定义邻居聚集函数（黑箱中是什么）
2. 定义结点嵌入上的损失函数$L(z_u)$
3. 对一系列的结点计算图进行训练
4. 生成结点嵌入

注意到$W_k$和$B_k$都是共享参数，因此模型参数的复杂度是关于$\vert V\vert$次线性的。同时，这种方法可以扩展到未见过的结点。

## GCN[^1]
关于GCN的详细介绍，可见论文一作的[博客](https://tkipf.github.io/graph-convolutional-networks/)。

依照上文的矩阵表示法，

$$H^{(k+1)}=f(H^{(k)},A)=\sigma(AH^{(k)}W^{(k)})$$

尽管该模型已经很强大，但是仍然存在以下两个问题，这也是文[^1]所要解决的：
1. 注意到$AH^{(k)}$消息聚合时并未将自身考虑进去，因此可以简单地给每个结点添加自环，即$\hat{A}=A+I$
2. 每个结点的度并不相同，导致$AH^{(k)}$计算时会改变特征向量的比例(scale)，那么简单的想法是类似PageRank一样对邻接阵$A$的每一行归一化，即$D^{-1}A$，其中$D$是度对角阵（取逆相当于对角线上元素，即每个结点的度数，直接取倒数）。而如果采用对称的归一化技术（即出入度都考虑），那就得到$D^{-1/2}AD^{-1/2}$。（对称阵的计算可见[图论基础]({% post_url 2020-02-24-graph-theory %})）

进而得到GCN的传播规则

$$H^{(k+1)}=f(H^{(k)},A)=\sigma\left(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(k)}W^{(k)}\right)$$

用向量形式来写即

$$\mathbf{h}_v^{(k+1)}=\sigma\left(\sum_{u\in N(v)\cup v}\frac{\mathbf{h}_u^{(k)}}{\sqrt{|N(u)||N(v)|}}W^{(k)}\right)$$

总的时间复杂度是$O(\vert E\vert)$。

GCN很强的一点在于，它还没训练就已经达到[DeepWalk]({% post_url 2020-02-06-graph-embedding %})的水准了。比如下图展示的是未训练的3层GCN结点嵌入情况，矩阵都是随机初始化的。

<table><tr>
<td><img src="https://tkipf.github.io/graph-convolutional-networks/images/karate.png"></td>
<td><img src="https://tkipf.github.io/graph-convolutional-networks/images/karate_emb.png"></td>
</tr></table>

至于为什么是卷积，想想卷积核在稀疏邻接矩阵上划过，聚合时实际上就相当于做卷积了。

## GraphSAGE[^2]
GraphSAGE (<u>SA</u>mple & aggre<u>G</u>at<u>E</u>)则是进一步将聚合的思想一般化，之前的聚合函数都是取平均，那现在我采用任意的聚合函数，得到

$$h_v^{(k+1)}=\sigma([W_k\cdot \mathop{AGG}(\{\mathbb{h}_{u}^{(k)},\forall u\in N(v)\}),B_k\mathbb{h}_v^{(k)}])$$

注意中括号表示两个向量（自嵌入+邻居嵌入）直接合并，邻居在GraphSAGE中并不取全部，而是随机取固定数目的邻居。

聚合函数就可以取
* 平均：最原始方案
$$\mathop{AGG}=\sum_{u\in N(u)}\frac{\mathbb{h}_{u}^{(k)}}{|N(v)|}$$
* 池化：$\gamma$为元素间(element-wise)平均/最值
$$\mathop{AGG}=\gamma(\{Q\mathbb{h}_{u}^{(k)},\forall u\in N(v)\})$$
* LSTM：$\pi$为随机置换(permutation)
$$\mathop{AGG}=LSTM([\mathbb{h}_{u}^{(k)},\forall u\in\pi(N(v))])$$

通常GCN和GraphSAGE只需两三层深即可。

从某种意义上，GCN有点像循环神经网络，因为GCN在每一层上都是共享参数的，而每一层就是一整个graph平展开来，每一层的计算都是传播一hop的计算，只有当这一层计算完了（图遍历完了），才进入到下一层的计算。

![gnn](https://d3i71xaburhd42.cloudfront.net/7a47891bc52c93c48c4a9309f61d5b16a2c5459c/3-Figure1-1.png)

从GraphSAGE的伪代码就更加能体会到这一点：外层做$K$次循环，内层遍历所有结点做消息传递（这其实相当于对整个无向图做一次BFS）

{% include image.html fig="Graph/graphsage-alg.png" width="80" %}


## Other Resources
* Stanford Network Analysis Project (SNAP): <http://snap.stanford.edu/>
* Geometric Deep Learning: <http://geometricdeeplearning.com/>
* Innovations in Graph Representation Learning: <https://ai.googleblog.com/2019/06/innovations-in-graph-representation.html>

## Reference
[^1]: Thomas N. Kipf, Max Welling (Amsterdam), *Semisupervised Classification with Graph Convolutional Networks*, ICLR, 2017 ([**Most cited publication**](https://github.com/naganandy/graph-based-deep-learning-literature))
[^2]: William L. Hamilton, Rex Ying, Jure Leskovec (Stanford), *Inductive Representation Learning on Large Graphs*, NeurIPS, 2017