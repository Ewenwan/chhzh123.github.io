---
layout: post
title: 图表示学习（2）-图神经网络
tags: [dl,graph]
---

这是**图表示学习(representation learning)的第二部分——图神经网络(graph neural network, gnn)**，主要涉及GCN [ICLR'17]和GraphSAGE [NeurIPS'17]两篇论文。

<!--more-->

关于图数据挖掘/表示学习的内容强烈建议去看Stanford [Jure Leskovec](https://cs.stanford.edu/people/jure/)的[Tutorial - Representation Learning on Networks (WWW'18)](http://snap.stanford.edu/proj/embeddings-www/)。

[前一节]({% post_url 2020-02-06-graph-embedding %})的图嵌入方法都是非常浅(shallow)的，通过学习得到一个矩阵/查找表，来得到结点的嵌入向量。但是这种方法有以下几点缺陷：
* 需要$O(\vert V\vert)$的参数量：没有任何参数的共享，所有结点都只有它自己的嵌入向量
* 内在传导(transductive)：很难生成一个没见过的结点的嵌入
* 无监督方法：没有将结点特征有机整合，而只是关注于图结构

因此我们希望采用更深的方法来得到结点嵌入，最好是直接学习得到一个连续函数，而不是表格映射（这个发展路径其实跟当年强化学习到深度强化学习的过程是类似的）。

核心思想是**从邻居结点整合特征**，通过黑箱网络，对输出拟合。如下图所示，0层即输入层，也即输入特征。

{% include image.html fig="Graph/grl-layers.png" width="80" %}

所以关键在于黑箱中是什么，也即**聚合信息**的方法。

最简单的方式，取平均然后过神经网络。

{% include image.html fig="Graph/grl-basic-math.png" width="80" %}

或用矩阵的形式表示（$H^{(0)}=X$）

$$H^{(k+1)}=f(H^{(k)},A)=\sigma(AH^{(k)}W^{(k)})$$

其中$A\in\mathbb{R}^{n\times n}$为邻接阵，$H\in\mathbb{R}^{n\times d}$为特征阵（注意这里一行为一个结点的嵌入向量，与前面的向量写法不同），$W\in\mathbb{R}^{d\times d}$为每一层的训练权重矩阵。$A$与$H$相乘相当于与$v_i$邻接的结点的第$j$个特征进行聚合，得到$(i,j)$矩阵元素。

这里就可以采用前面的无监督方法作为损失函数，或者用有监督的模型来构造。

总的来说，GNN的模型构造分为以下四步走：
1. 定义邻居聚集函数（黑箱中是什么）
2. 定义结点嵌入上的损失函数$L(z_u)$
3. 对一系列的结点计算图进行训练
4. 生成结点嵌入

注意到$W_k$和$B_k$都是共享参数，因此模型参数的复杂度是关于$\vert V\vert$次线性的。同时，这种方法可以扩展到未见过的结点。

## GCN[^1]
关于GCN的详细介绍，可见论文一作的[博客](https://tkipf.github.io/graph-convolutional-networks/)。

依照上文的矩阵表示法，

$$H^{(k+1)}=f(H^{(k)},A)=\sigma(AH^{(k)}W^{(k)})$$

尽管该模型已经很强大，但是仍然存在以下两个问题，这也是文[^1]所要解决的：
1. 注意到$AH^{(k)}$聚合特征时并未将自身考虑进去，因此可以简单给每个结点添加自环，即$\hat{A}=A+I$
2. 每个结点的度并不相同，导致$AH^{(k)}$计算时会改变特征向量的比例(scale)，那么简单的想法是类似PageRank一样对邻接阵$A$的每一行归一化，即$D^{-1}A$，其中$D$是度对角阵（取逆相当于对角线上元素，即每个结点的度数，直接取倒数）。而如果采用对称的归一化技术（即出入度都考虑），那就得到$D^{-1/2}AD^{-1/2}$。

进而得到GCN的传播规则

$$H^{(k+1)}=f(H^{(k)},A)=\sigma\left(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(k)}W^{(k)}\right)$$

用向量形式来写即

$$h_v^{(k+1)}=\sigma\left(W_k\sum_{u\in N(v)\cup v}\frac{\mathbb{h}_u^{(k)}}{\sqrt{|N(u)||N(v)|}}\right)$$

总的时间复杂度是$O(\vert E\vert)$。

GCN很强的一点在于，它还没训练就已经达到[DeepWalk]({% post_url 2020-02-06-graph-embedding %})的水准了。比如下图展示的是未训练的3层GCN结点嵌入情况，矩阵都是随机初始化的。

<table><tr>
<td><img src="https://tkipf.github.io/graph-convolutional-networks/images/karate.png"></td>
<td><img src="https://tkipf.github.io/graph-convolutional-networks/images/karate_emb.png"></td>
</tr></table>

至于为什么是卷积，想想卷积核在稀疏邻接矩阵上划过，聚合时实际上就相当于做卷积了。

## GraphSAGE[^2]
GraphSAGE (<u>SA</u>mple & aggre<u>G</u>at<u>E</u>)则是进一步将聚合的思想一般化，之前的聚合函数都是取平均，那现在我采用任意的聚合函数，得到

$$h_v^{(k+1)}=\sigma([W_k\cdot \mathop{AGG}(\{\mathbb{h}_{u}^{(k)},\forall u\in N(v)\}),B_k\mathbb{h}_v^{(k)}])$$

注意中括号表示两个向量（自嵌入+邻居嵌入）直接合并，邻居在GraphSAGE中并不取全部，而是随机取固定数目的邻居。

聚合函数就可以取
* 平均：最原始方案
$$\mathop{AGG}=\sum_{u\in N(u)}\frac{\mathbb{h}_{u}^{(k)}}{|N(v)|}$$
* 池化：$\gamma$为元素间(element-wise)平均/最值
$$\mathop{AGG}=\gamma(\{Q\mathbb{h}_{u}^{(k)},\forall u\in N(v)\})$$
* LSTM：$\pi$为随机置换(permutation)
$$\mathop{AGG}=LSTM([\mathbb{h}_{u}^{(k)},\forall u\in\pi(N(v))])$$

通常GCN和GraphSAGE只需两三层深即可。

从某种意义上，GCN有点像循环神经网络，因为GCN在每一层上都是共享参数的，而每一层就是一整个graph平展开来，每一层的计算都是传播一hop的计算，只有当这一层计算完了（图遍历完了），才进入到下一层的计算。

![gnn](https://d3i71xaburhd42.cloudfront.net/7a47891bc52c93c48c4a9309f61d5b16a2c5459c/3-Figure1-1.png)

从GraphSAGE的伪代码就更加能体会到这一点：外层做$K$次循环，内层遍历所有结点做消息传递（这其实相当于对整个无向图做一次BFS）

{% include image.html fig="Graph/graphsage-alg.png" width="80" %}


## Other Resources
* Stanford Network Analysis Project (SNAP): <http://snap.stanford.edu/>
* Geometric Deep Learning: <http://geometricdeeplearning.com/>
* Innovations in Graph Representation Learning: <https://ai.googleblog.com/2019/06/innovations-in-graph-representation.html>

## Reference
[^1]: Thomas N. Kipf, Max Welling (Amsterdam), *Semisupervised Classification with Graph Convolutional Networks*, ICLR, 2017 ([**Most cited publication**](https://github.com/naganandy/graph-based-deep-learning-literature))
[^2]: William L. Hamilton, Rex Ying, Jure Leskovec (Stanford), *Inductive Representation Learning on Large Graphs*, NeurIPS, 2017