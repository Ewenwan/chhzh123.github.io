---
layout: post
title: 图表示学习（2）-图神经网络
tags: [dl,graph]
---

这是**图表示学习(representation learning)的第二部分——图神经网络(graph neural network, gnn)**，主要涉及GCN [ICLR'17]、GraphSAGE [NeurIPS'17]、NeuGraph [ATC'19]和 AliGraph [VLDB'19]四篇论文。

<!--more-->

关于图数据挖掘/表示学习的内容强烈建议去看Stanford [Jure Leskovec](https://cs.stanford.edu/people/jure/)的[Tutorial - Representation Learning on Networks (WWW'18)](http://snap.stanford.edu/proj/embeddings-www/)。

[前一节]({% post_url 2020-02-06-graph-embedding %})的图嵌入方法都是非常浅(shallow)的，通过学习得到一个矩阵/查找表，来得到结点的嵌入向量。但是这种方法有以下几点缺陷：
* 需要$O(\vert V\vert)$的参数量：没有任何参数的共享，所有结点都只有它自己的嵌入向量
* 内在传导(transductive)：很难生成一个没见过的结点的嵌入
* 无监督方法：没有将结点特征有机整合，而只是关注于图结构

因此我们希望采用更深的方法来得到结点嵌入，最好是直接学习得到一个连续函数，而不是表格映射（这个发展路径其实跟当年强化学习到深度强化学习的过程是类似的）。

核心思想是**从邻居结点整合特征**，通过黑箱网络，对输出拟合。如下图所示，0层即输入层，也即输入特征。

{% include image.html fig="Graph/grl-layers.png" width="80" %}

所以关键在于黑箱中是什么，也即**聚合信息**的方法。

最简单的方式，取平均然后过神经网络。

{% include image.html fig="Graph/grl-basic-math.png" width="80" %}

或用矩阵的形式表示（$H^{(0)}=X$）

$$H^{(k+1)}=f(H^{(k)},A)=\sigma(AH^{(k)}W^{(k)})$$

其中$A\in\mathbb{R}^{n\times n}$为邻接阵，$H\in\mathbb{R}^{n\times d}$为特征阵（注意这里一行为一个结点的嵌入向量，与前面的向量写法不同），$W\in\mathbb{R}^{d\times d}$为每一层的训练权重矩阵。$A$与$H$相乘相当于与$v_i$邻接的结点的第$j$个特征进行聚合，得到$(i,j)$矩阵元素。

这里就可以采用前面的无监督方法作为损失函数，或者用有监督的模型来构造。

总的来说，GNN的模型构造分为以下四步走：
1. 定义邻居聚集函数（黑箱中是什么）
2. 定义结点嵌入上的损失函数$L(z_u)$
3. 对一系列的结点计算图进行训练
4. 生成结点嵌入

注意到$W_k$和$B_k$都是共享参数，因此模型参数的复杂度是关于$\vert V\vert$次线性的。同时，这种方法可以扩展到未见过的结点。

## GCN[^1]
关于GCN的详细介绍，可见论文一作的[博客](https://tkipf.github.io/graph-convolutional-networks/)。

依照上文的矩阵表示法，

$$H^{(k+1)}=f(H^{(k)},A)=\sigma(AH^{(k)}W^{(k)})$$

尽管该模型已经很强大，但是仍然存在以下两个问题，这也是文[^1]所要解决的：
1. 注意到$AH^{(k)}$聚合特征时并未将自身考虑进去，因此可以简单给每个结点添加自环，即$\hat{A}=A+I$
2. 每个结点的度并不相同，导致$AH^{(k)}$计算时会改变特征向量的比例(scale)，那么简单的想法是类似PageRank一样对邻接阵$A$的每一行归一化，即$D^{-1}A$，其中$D$是度对角阵（取逆相当于对角线上元素，即每个结点的度数，直接取倒数）。而如果采用对称的归一化技术（即出入度都考虑），那就得到$D^{-1/2}AD^{-1/2}$。

进而得到GCN的传播规则

$$H^{(k+1)}=f(H^{(k)},A)=\sigma(\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(k)}W^{(k)})$$

用向量形式来写即

$$h_v^{(k+1)}=\sigma\left(W_k\sum_{u\in N(v)\cup v}\frac{\mathbb{h}_u^{(k)}}{\sqrt{|N(u)||N(v)|}}\right)$$

总的时间复杂度是$O(\vert E\vert)$$。

GCN很强的一点在于，它还没训练就已经达到[DeepWalk]({% post_url 2020-02-06-graph-embedding %})的水准了。比如下图展示的是未训练的3层GCN结点嵌入情况，矩阵都是随机初始化的。

<table><tr>
<td><img src="https://tkipf.github.io/graph-convolutional-networks/images/karate.png"></td>
<td><img src="https://tkipf.github.io/graph-convolutional-networks/images/karate_emb.png"></td>
</tr></table>


## GraphSAGE[^2]
GraphSAGE (<u>SA</u>mple & aggre<u>G</u>at<u>E</u>)则是进一步将聚合的思想一般化，之前的聚合函数都是取平均，那现在我采用任意的聚合函数，得到

$$h_v^{(k+1)}=\sigma([W_k\cdot \mathop{AGG}(\{\mathbb{h}_{u}^{(k)},\forall u\in N(v)\}),B_k\mathbb{h}_v^{(k)}])$$

注意中括号表示两个向量（自嵌入+邻居嵌入）直接合并，邻居在GraphSAGE中并不取全部，而是随机取固定数目的邻居。

聚合函数就可以取
* 平均：最原始方案
$$\mathop{AGG}=\sum_{u\in N(u)}\frac{\mathbb{h}_{u}^{(k)}}{|N(v)|}$$
* 池化：$\gamma$为元素间(element-wise)平均/最值
$$\mathop{AGG}=\gamma(\{Q\mathbb{h}_{u}^{(k)},\forall u\in N(v)\})$$
* LSTM：$\pi$为随机置换(permutation)
$$\mathop{AGG}=LSTM([\mathbb{h}_{u}^{(k)},\forall u\in\pi(N(v))])$$

通常GCN和GraphSAGE只需两三层深即可。

从某种意义上，GCN有点像循环神经网络，因为GCN在每一层上都是共享参数的，而每一层就是一整个graph平展开来，每一层的计算都是传播一hop的计算，只有当这一层计算完了（图遍历完了），才进入到下一层的计算。

![gnn](https://d3i71xaburhd42.cloudfront.net/7a47891bc52c93c48c4a9309f61d5b16a2c5459c/3-Figure1-1.png)

从GraphSAGE的伪代码就更加能体会到这一点：外层做$K$次循环，内层遍历所有结点做消息传递（这其实相当于对整个无向图做一次BFS）

{% include image.html fig="Graph/graphsage-alg.png" width="80" %}

## NeuGraph[^3]
**深度学习系统最大问题在于没有办法高效表示图数据，而图系统最大的问题在于没法自动微分！**

现有用得最广泛的框架是[DGL (Deep Graph Library)](https://github.com/dmlc/dgl)，但DGL只是提供了一个编程框架（面向图的消息传递模型），并没有深度解决计算的问题（这很大程度也是GCN很难火起来的原因，因为无法做到很高的可扩展性）。在GCN的原作[实现](https://github.com/tkipf/gcn)和GraphSAGE的原作[实现](https://github.com/williamleif/GraphSAGE)中，都使用了TensorFlow进行编程，但是他们所采用的方法都是简单暴力的矩阵乘，这样其实很大程度忽略了图计算框架这些年取得的成果。因此NeuGraph的出现也正是为了弥合这两者，将图计算与深度学习有机地融合起来。（某种意义上，这也是matrix-based和matrix-free两种方法的对碰。）

（之前以为腾讯的[Plato](https://github.com/Tencent/plato)作为企业级图系统，应该可以支持GNN的计算，然而Plato只是将[Gemini](https://github.com/thu-pacman/GeminiGraph)和[KnightKing]({% post_url 2020-02-06-graph-embedding %})套了个壳，所以目前也只支持这两个框架所支持的算法，即传统的图算法以及随机游走。）

NeuGraph把常见的GNN分为三类：图卷积、图循环、图注意力网络。
进而提出了SAGA-NN (Scatter-ApplyEdge-Gather-ApplyVertex with Neural Networks)编程模型，其中<u>S</u>A<u>G</u>A部分属于图计算的消息传递，而两个A则是深度学习神经网络的应用。

![saga-nn](https://d3i71xaburhd42.cloudfront.net/7a47891bc52c93c48c4a9309f61d5b16a2c5459c/4-Figure2-1.png)

由于GCN相比起传统的图算法来说（在图计算层面上）要简单很多，就是对全图不断进行遍历，因此Scatter和Gather是确定的，而两个Apply阶段则是用户自定义的函数。（所以似乎NeuGraph没法实现GraphSAGE，因为GraphSAGE的邻居是由一定策略采样出来的，而不是取全部邻居）。

### GPU Execution
目前的深度学习框架都很难处理大图，因为GPU的内存无法存储这么大规模的图，因此NeuGraph在数据流抽象的基础上进行了图划分。

（关于计算硬件，这里是值得考虑的。GPU在稠密矩阵计算上具有先天优势，但如果换成稀疏阵优势是否还存在呢。图处理框架的发展证明了CPU集群有办法承担大规模的图计算任务，从这种角度来看的话是否CPU在GNN的处理上也更存在优势呢？或者更加激进地，利用FPGA实现这样既能高效遍历又能高效算矩阵的架构是否有办法呢？）

简而言之，按边划分为chunk（准确来说是把邻接阵按列划分），然后送到不同的GPU上进行计算，下面补充了几点文中提到的优化方法(streaming out of GPU core)：
* 使用selective scheduling，先用CPU筛一遍有用的边，再把这些边送去GPU算
* 为了确保足够的局部性，采用了Kernighan-Lin算法进行图划分（METIS包），确保同一个chunk中的大部分边都连向同一个节点
* 用pipeline scheduling最大程度重叠IO和计算时间

### Experiments
NeuGraph在TensorFlow上实现，包含5000行的C++代码和3000行的Python代码。实验服务器2*28核CPU+512G内存+8块NVIDIA Tesla P100 GPU。最大的数据集为Amazon，8.6M的顶点和231.6M的边，特征96，类别22。

比较对象包括TensorFlow、GraphSAGE、DGL，提速比在2.5倍到8.1倍之间（单GPU），但没有办法跟多GPU的Tensorflow比，因为直接爆内存了。而不加优化的多GPU版TF-SAGA大概比NerGraph慢2.4到4.9倍，这一部分才比较客观地表明提出的优化方法的高效性。

## AliGraph[^4]


## Reference
[^1]: Thomas N. Kipf, Max Welling (Amsterdam), *Semisupervised Classification with Graph Convolutional Networks*, ICLR, 2017 ([**Most cited publication**](https://github.com/naganandy/graph-based-deep-learning-literature))
[^2]: William L. Hamilton, Rex Ying, Jure Leskovec (Stanford), *Inductive Representation Learning on Large Graphs*, NeurIPS, 2017
[^3]: Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong Zhou (MSRA), Yafei Dai (PKU), *NeuGraph: Parallel Deep Neural Network Computation on Large Graphs*, ATC, 2019
[^4]: Rong Zhu, Kun Zhao, Hongxia Yang, Wei Lin, Chang Zhou, Baole Ai, Yong Li, Jingren Zhou (Alibaba), *AliGraph: A Comprehensive Graph Neural Network Platform*, VLDB, 2019

## Other Resources
* Stanford Network Analysis Project (SNAP): <http://snap.stanford.edu/>
* Geometric Deep Learning: <http://geometricdeeplearning.com/>
* Innovations in Graph Representation Learning: <https://ai.googleblog.com/2019/06/innovations-in-graph-representation.html>