---
layout: post
title: Binary Neural Network (BNN)
tags: [dl]
---

本文简要记录二值神经网络(BNN)的基本原理及计算方式。

<!--more-->

## 基本原理
BNN与CNN最大的区别在于**矩阵乘法**的处理，也就是<u>卷积层</u>和<u>全连接层</u>，都采用**量化**的方式，如下用+1和-1两个值来表示。

| $\mathbf{x}$ | $\mathbf{y}$ | $\mathbf{x}\cdot\mathbf{y}$ |
| :--: | :--: | :--: |
| -1 | -1 | +1 |
| -1 | +1 | -1 |
| +1 | -1 | -1 |
| +1 | +1 | +1 |

为了方便硬件实现，可改写为下面的同或(XNOR)操作。

| $\hat{\mathbf{x}}$ | $\hat{\mathbf{y}}$ | $\hat{\mathbf{x}}\odot\hat{\mathbf{y}}$ |
| :--: | :--: | :--: |
| 0 | 0 | 1 |
| 0 | 1 | 0 |
| 1 | 0 | 0 |
| 1 | 1 | 1 |

在算向量乘法时即依据下面的计算公式

$$\begin{aligned}
\mathbf{x}\cdot\mathbf{y}
&=\sum_{i=0}^L x_i\cdot y_i\\
&=\sum_{i=0}^L (2(\hat{x}_i\odot \hat{y}_i)-1)\\
&=2\sum_{i=0}^L (\hat{x}_i\odot\hat{y}_i) - L
\end{aligned}$$

所以实际上二值神经网络并不是真的二值，仅仅是网络的**权重**采用二值表示，其余计算依然会涉及到整数及浮点数。

### Conv
{% include image.html fig="HeteroCL/bnn_conv.jpg" width="80" %}

即[MAC (multiply-accumulate)](https://en.wikipedia.org/wiki/Multiply%E2%80%93accumulate_operation)操作

$$a\leftarrow a+(b\times c)$$

对应的有**融合乘加(fused multiply-add, FMA)**操作，这在CPU SIMD中很常见。

对于这种卷积操作，硬件可以很方便地通过XNOR和popcount实现。
![xnor](https://software.intel.com/sites/default/files/managed/c0/e0/webops10048-fig3-bit-packing.png)

注意padding之后的元素并不算入popcount当中，因此在reduce求和之前需要先判断是否在padding的边界。

### Max pooling
池化后的值大于0取1，否则取0。

### Dense
同XNOR方式的矩阵乘，注意matmul的结果是**有符号整数值**，但bias是浮点数，故加完bias会变**实数**。

如果采用ReLU进行激活，则大于0归为1，否则归为0（**二值**）。

### Batch norm
通常为了让神经网络不受批次影响，有更好的泛化能力，会采用类似CNN的Batch norm方法来做批归一化操作。

$$\mathbf{y}=\frac{\mathbf{x}-\mathbf{\mu}}{\sqrt{\sigma^2+\varepsilon}}\gamma+\beta$$

归一化后做二值化，大于0取1，否则取0。

在具体实现这些层时一定要注意每一层的输出结果的**类型**，否则做量化时就会出错。

## References
* [SDSoC Tutorial](https://github.com/seanlatias/sdsoc-tutorial/tree/master/bnn)
* [Cornell 5775 Lab 4](https://www.csl.cornell.edu/courses/ece5775/pdf/lab4.pdf)
* [HeteroCL BNN Implementation](https://github.com/chhzh123/heterocl/blob/mydev-v0.3/hlib/python/hlib/op/bnn.py)
* [HeteroCL BNN Demo](https://github.com/chhzh123/heterocl-demo/blob/master/bnn/bnn_main.py)